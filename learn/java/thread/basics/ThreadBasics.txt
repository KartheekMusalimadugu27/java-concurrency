Program#
A program is a set of instructions and associated data that resides on the disk and is loaded by the operating system to 
perform some task. An executable file or a python script file are examples of programs. In order to run a program, 
the operating system's kernel is first asked to create a new process, which is an environment in which a program executes.


Process#
A process is a program in execution. A process is an execution environment that consists of instructions, user-data, 
and system-data segments, as well as lots of other resources such as CPU, memory, address-space, disk and network I/O 
acquired at runtime. A program can have several copies of it running at the same time but a process necessarily belongs 
to only one program.

Thread#
Thread is the smallest unit of execution in a process. A thread simply executes instructions serially. A process can 
have multiple threads running as part of it. Usually, there would be some state associated with the process that is 
shared among all the threads and in turn each thread would have some state private to itself. The globally shared 
state amongst the threads of a process is visible and accessible to all the threads, and special attention needs to 
be paid when any thread tries to read or write to this global shared state. There are several constructs offered by 
various programming languages to guard and discipline the access to this global state, which we will go into further
detail in upcoming lessons.

------------------------------------------------------------------------------------------------------------------------------

Concurrency vs Parallelism#
From the above discussion it should be apparent that a concurrent system need not be parallel, whereas a parallel system 
is indeed concurrent. Additionally, a system can be both concurrent and parallel e.g. a multitasking operating system 
running on a multicore machine.

Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once. Last but 
not the least, you'll find literature describing concurrency as a property of a program or a system whereas parallelism 
as a runtime behaviour of executing multiple tasks.

We end the lesson with an analogy, frequently quoted in online literature, of customers waiting in two queues to buy 
coffee. Single-processor concurrency is akin to alternatively serving customers from the two queues but with a single 
coffee machine, while parallelism is similar to serving each customer queue with a dedicated coffee machine.

-----------------------------------------------------------------------------------------------------------------------------

Preemptive Multitasking# (Core Feature of all Unix Based Systems)
In preemptive multitasking, the operating system preempts a program to allow another waiting task to run on the CPU. 
Programs or threads can't decide how long for or when they can use the CPU. The operating system’s scheduler decides 
which thread or program gets to use the CPU next and for how much time. Furthermore, scheduling of programs or threads 
on the CPU isn’t predictable. A thread or program once taken off of the CPU by the scheduler can't determine when it 
will get on the CPU next. As a consequence, if a malicious program initiates an infinite loop, it only hurts itself 
without affecting other programs or threads. Lastly, the programmer isn't burdened to decide when to give up control 
back to the CPU in code.

Cooperative Multitasking#
Cooperative Multitasking involves well-behaved programs to voluntarily give up control back to the scheduler so that 
another program can run. A program or thread may give up control after a period of time has expired or if it becomes 
idle or logically blocked. The operating system’s scheduler has no say in how long a program or thread runs for. 
A malicious program can bring the entire system to a halt by busy waiting or running an infinite loop and not giving 
up control. The process scheduler for an operating system implementing cooperative multitasking is called a 
cooperative scheduler. As the name implies, the participating programs or threads are required to cooperate to 
make the scheduling scheme work.

----------------------------------------------------------------------------------------------------------------------------

Synchronous#
Synchronous execution refers to line-by-line execution of code. If a function is invoked, the program execution 
waits until the function call is completed. Synchronous execution blocks at each method call before proceeding to 
the next line of code. A program executes in the same sequence as the code in the source code file. Synchronous 
execution is synonymous to serial execution.

Asynchronous#
Asynchronous (or async) execution refers to execution that doesn't block when invoking subroutines. Or if you 
prefer the more fancy Wikipedia definition: Asynchronous programming is a means of parallel programming in which 
a unit of work runs separately from the main application thread and notifies the calling thread of its completion, 
failure or progress. An asynchronous program doesn’t wait for a task to complete and can move on to the next task.

----------------------------------------------------------------------------------------------------------------------------

A program running on your machine will broadly require:

CPU Time
Memory
Networking Resources
Disk Storage

----------------------------------------------------------------------------------------------------------------------------

CPU Bound#
Programs which are compute-intensive i.e. program execution requires very high utilization of the CPU (close to 100%) 
are called CPU bound programs. Such programs primarily depend on improving CPU speed to decrease program completion 
time. This could include programs such as data crunching, image processing, matrix multiplication etc.

I/O Bound#
I/O bound programs are the opposite of CPU bound programs. Such programs spend most of their time waiting for input 
or output operations to complete while the CPU sits idle. I/O operations can consist of operations that write or 
read from main memory or network interfaces. Because the CPU and main memory are physically separate a data bus 
exists between the two to transfer bits to and fro. Similarly, data needs to be moved between network interfaces 
and CPU/memory. Even though the physical distances are tiny, the time taken to move the data across is big enough 
for several thousand CPU cycles to go waste. This is why I/O bound programs would show relatively lower CPU utilization 
than CPU bound programs.

----------------------------------------------------------------------------------------------------------------------------

Throughput#
Throughput is defined as the rate of doing work or how much work gets done per unit of time. If you are an Instagram user, 
you could define throughput as the number of images your phone or browser downloads per unit of time.

Latency#
Latency is defined as the time required to complete a task or produce a result. Latency is also referred to as response 
time. The time it takes for a web browser to download Instagram images from the internet is the latency for downloading 
the images.

---------------------------------------------------------------------------------------------------------------------------

Critical Section#
Critical section is any piece of code that has the possibility of being executed concurrently by more than one thread 
of the application and exposes any shared data or resources used by the application for access.

Race Condition#
Race conditions happen when threads run through critical sections without thread synchronization. The threads 
"race" through the critical section to write or read shared resources and depending on the order in which threads 
finish the "race", the program output changes. In a race condition, threads access shared resources or program 
variables that might be worked on by other threads at the same time causing the application data to be inconsistent.

-----------------------------------------------------------------------------------------------------------------------------

Logical follies committed in multithreaded code, while trying to avoid race conditions and guarding critical sections, 
can lead to a host of subtle and hard to find bugs and side-effects. Some of these incorrect usage patterns have their 
names and are discussed below.

DeadLock#
Deadlocks occur when two or more threads aren't able to make any progress because the resource required by the first 
thread is held by the second and the resource required by the second thread is held by the first.

Liveness#
Ability of a program or an application to execute in a timely manner is called liveness. If a program experiences a 
deadlock then it's not exhibiting liveness.

Live-Lock#
A live-lock occurs when two threads continuously react in response to the actions by the other thread without making 
any real progress. The best analogy is to think of two persons trying to cross each other in a hallway. John moves to 
the left to let Arun pass, and Arun moves to his right to let John pass. Both block each other now. John sees he's 
blocking Arun again and moves to his right and Arun moves to his left seeing he's blocking John. They never cross 
each other and keep blocking each other. This scenario is an example of a livelock. A process seems to be running 
and not deadlocked but in reality, isn't making any progress.

Starvation#
Other than a deadlock, an application thread can also experience starvation, when it never gets CPU time or access 
to shared resources. Other greedy threads continuously hog shared system resources not letting the starving thread 
make any progress.

Reentrant Lock#
Re-entrant locks allow for re-locking or re-entering of a synchronization lock. 
If a synchronization primitive doesn't allow reacquisition of itself by a thread that has already acquired it, 
then such a thread would block as soon as it attempts to reacquire the primitive a second time.

-----------------------------------------------------------------------------------------------------------------------------

Mutex#
Mutex as the name hints implies mutual exclusion. A mutex is used to guard shared data such as a linked-list, 
an array or any primitive type. A mutex allows only a single thread to access a resource or critical section.

Once a thread acquires a mutex, all other threads attempting to acquire the same mutex are blocked until the 
first thread releases the mutex. Once released, most implementations arbitrarily chose one of the waiting 
threads to acquire the mutex and make progress.

Semaphore#
Semaphore, on the other hand, is used for limiting access to a collection of resources. Think of semaphore 
s having a limited number of permits to give out. If a semaphore has given out all the permits it has, then 
any new thread that comes along requesting for a permit will be blocked, till an earlier thread with a permit 
returns it to the semaphore. A typical example would be a pool of database connections that can be handed out 
to requesting threads. Say there are ten available connections but 50 requesting threads. In such a scenario, 
a semaphore can only give out ten permits or connections at any given point in time.

A semaphore with a single permit is called a binary semaphore and is often thought of as an equivalent of a 
mutex, which isn't completely correct as we'll shortly explain. Semaphores can also be used for signaling 
among threads. This is an important distinction as it allows threads to cooperatively work towards completing 
a task. A mutex, on the other hand, is strictly limited to serializing access to shared state among competing 
threads.

-------------------------------------------------------------------------------------------------------------------

Difference between Mutex and Semaphore#

Mutex implies mutual exclusion and is used to serialize access to critical sections whereas semaphore can 
potentially be used as a mutex but it can also be used for cooperation and signaling amongst threads. 
Semaphore also solves the issue of missed signals.

Mutex is owned by a thread, whereas a semaphore has no concept of ownership.

Mutex if locked, must necessarily be unlocked by the same thread. A semaphore can be acted upon by different 
threads. This is true even if the semaphore has a permit of one

Think of semaphore analogous to a car rental service such as Hertz. Each outlet has a certain number of cars, 
it can rent out to customers. It can rent several cars to several customers at the same time but if all the 
cars are rented out then any new customers need to be put on a waitlist till one of the rented cars is returned. 
In contrast, think of a mutex like a lone runway on a remote airport. Only a single jet can land or take-off 
from the runway at a given point in time. No other jet can use the runway simultaneously with the first aircraft.

-----------------------------------------------------------------------------------------------------------------------

Java’s Monitor#
In Java every object is a condition variable and has an associated lock that is hidden from the developer. 
Each java object exposes wait() and notify() methods.

Before we execute wait() on a java object we need to lock its hidden mutex. That is done implicitly through 
the synchronized keyword. If you attempt to call wait() or notify() outside of a synchronized block, an 
IllegalMonitorStateException would occur. It's Java reminding the developer that the mutex wasn't acquired 
before wait on the condition variable was invoked. wait() and notify() can only be called on an object once 
the calling thread becomes the owner of the monitor. The ownership of the monitor can be achieved in the 
following ways:

1) the method the thread is executing has synchronized in its signature

2) the thread is executing a block that is synchronized on the object on which wait or notify will be called

3) in case of a class, the thread is executing a static method which is synchronized.

----------------------------------------------------------------------------------------------------------------------

