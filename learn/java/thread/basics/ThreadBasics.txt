Program#
A program is a set of instructions and associated data that resides on the disk and is loaded by the operating system to 
perform some task. An executable file or a python script file are examples of programs. In order to run a program, 
the operating system's kernel is first asked to create a new process, which is an environment in which a program executes.


Process#
A process is a program in execution. A process is an execution environment that consists of instructions, user-data, 
and system-data segments, as well as lots of other resources such as CPU, memory, address-space, disk and network I/O 
acquired at runtime. A program can have several copies of it running at the same time but a process necessarily belongs 
to only one program.

Thread#
Thread is the smallest unit of execution in a process. A thread simply executes instructions serially. A process can 
have multiple threads running as part of it. Usually, there would be some state associated with the process that is 
shared among all the threads and in turn each thread would have some state private to itself. The globally shared 
state amongst the threads of a process is visible and accessible to all the threads, and special attention needs to 
be paid when any thread tries to read or write to this global shared state. There are several constructs offered by 
various programming languages to guard and discipline the access to this global state, which we will go into further
detail in upcoming lessons.

------------------------------------------------------------------------------------------------------------------------------

Concurrency vs Parallelism#
From the above discussion it should be apparent that a concurrent system need not be parallel, whereas a parallel system 
is indeed concurrent. Additionally, a system can be both concurrent and parallel e.g. a multitasking operating system 
running on a multicore machine.

Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once. Last but 
not the least, you'll find literature describing concurrency as a property of a program or a system whereas parallelism 
as a runtime behaviour of executing multiple tasks.

We end the lesson with an analogy, frequently quoted in online literature, of customers waiting in two queues to buy 
coffee. Single-processor concurrency is akin to alternatively serving customers from the two queues but with a single 
coffee machine, while parallelism is similar to serving each customer queue with a dedicated coffee machine.

-----------------------------------------------------------------------------------------------------------------------------

Preemptive Multitasking# (Core Feature of all Unix Based Systems)
In preemptive multitasking, the operating system preempts a program to allow another waiting task to run on the CPU. 
Programs or threads can't decide how long for or when they can use the CPU. The operating system’s scheduler decides 
which thread or program gets to use the CPU next and for how much time. Furthermore, scheduling of programs or threads 
on the CPU isn’t predictable. A thread or program once taken off of the CPU by the scheduler can't determine when it 
will get on the CPU next. As a consequence, if a malicious program initiates an infinite loop, it only hurts itself 
without affecting other programs or threads. Lastly, the programmer isn't burdened to decide when to give up control 
back to the CPU in code.

Cooperative Multitasking#
Cooperative Multitasking involves well-behaved programs to voluntarily give up control back to the scheduler so that 
another program can run. A program or thread may give up control after a period of time has expired or if it becomes 
idle or logically blocked. The operating system’s scheduler has no say in how long a program or thread runs for. 
A malicious program can bring the entire system to a halt by busy waiting or running an infinite loop and not giving 
up control. The process scheduler for an operating system implementing cooperative multitasking is called a 
cooperative scheduler. As the name implies, the participating programs or threads are required to cooperate to 
make the scheduling scheme work.

----------------------------------------------------------------------------------------------------------------------------

Synchronous#
Synchronous execution refers to line-by-line execution of code. If a function is invoked, the program execution 
waits until the function call is completed. Synchronous execution blocks at each method call before proceeding to 
the next line of code. A program executes in the same sequence as the code in the source code file. Synchronous 
execution is synonymous to serial execution.

Asynchronous#
Asynchronous (or async) execution refers to execution that doesn't block when invoking subroutines. Or if you 
prefer the more fancy Wikipedia definition: Asynchronous programming is a means of parallel programming in which 
a unit of work runs separately from the main application thread and notifies the calling thread of its completion, 
failure or progress. An asynchronous program doesn’t wait for a task to complete and can move on to the next task.

----------------------------------------------------------------------------------------------------------------------------

A program running on your machine will broadly require:

CPU Time
Memory
Networking Resources
Disk Storage

----------------------------------------------------------------------------------------------------------------------------

CPU Bound#
Programs which are compute-intensive i.e. program execution requires very high utilization of the CPU (close to 100%) 
are called CPU bound programs. Such programs primarily depend on improving CPU speed to decrease program completion 
time. This could include programs such as data crunching, image processing, matrix multiplication etc.

I/O Bound#
I/O bound programs are the opposite of CPU bound programs. Such programs spend most of their time waiting for input 
or output operations to complete while the CPU sits idle. I/O operations can consist of operations that write or 
read from main memory or network interfaces. Because the CPU and main memory are physically separate a data bus 
exists between the two to transfer bits to and fro. Similarly, data needs to be moved between network interfaces 
and CPU/memory. Even though the physical distances are tiny, the time taken to move the data across is big enough 
for several thousand CPU cycles to go waste. This is why I/O bound programs would show relatively lower CPU utilization 
than CPU bound programs.

----------------------------------------------------------------------------------------------------------------------------

Throughput#
Throughput is defined as the rate of doing work or how much work gets done per unit of time. If you are an Instagram user, 
you could define throughput as the number of images your phone or browser downloads per unit of time.

Latency#
Latency is defined as the time required to complete a task or produce a result. Latency is also referred to as response 
time. The time it takes for a web browser to download Instagram images from the internet is the latency for downloading 
the images.

---------------------------------------------------------------------------------------------------------------------------

Critical Section#
Critical section is any piece of code that has the possibility of being executed concurrently by more than one thread 
of the application and exposes any shared data or resources used by the application for access.

Race Condition#
Race conditions happen when threads run through critical sections without thread synchronization. The threads 
"race" through the critical section to write or read shared resources and depending on the order in which threads 
finish the "race", the program output changes. In a race condition, threads access shared resources or program 
variables that might be worked on by other threads at the same time causing the application data to be inconsistent.

-----------------------------------------------------------------------------------------------------------------------------

Logical follies committed in multithreaded code, while trying to avoid race conditions and guarding critical sections, 
can lead to a host of subtle and hard to find bugs and side-effects. Some of these incorrect usage patterns have their 
names and are discussed below.

DeadLock#
Deadlocks occur when two or more threads aren't able to make any progress because the resource required by the first 
thread is held by the second and the resource required by the second thread is held by the first.

Liveness#
Ability of a program or an application to execute in a timely manner is called liveness. If a program experiences a 
deadlock then it's not exhibiting liveness.

Live-Lock#
A live-lock occurs when two threads continuously react in response to the actions by the other thread without making 
any real progress. The best analogy is to think of two persons trying to cross each other in a hallway. John moves to 
the left to let Arun pass, and Arun moves to his right to let John pass. Both block each other now. John sees he's 
blocking Arun again and moves to his right and Arun moves to his left seeing he's blocking John. They never cross 
each other and keep blocking each other. This scenario is an example of a livelock. A process seems to be running 
and not deadlocked but in reality, isn't making any progress.

Starvation#
Other than a deadlock, an application thread can also experience starvation, when it never gets CPU time or access 
to shared resources. Other greedy threads continuously hog shared system resources not letting the starving thread 
make any progress.

Reentrant Lock#
Re-entrant locks allow for re-locking or re-entering of a synchronization lock. 
If a synchronization primitive doesn't allow reacquisition of itself by a thread that has already acquired it, 
then such a thread would block as soon as it attempts to reacquire the primitive a second time.

-----------------------------------------------------------------------------------------------------------------------------

Mutex#
Mutex as the name hints implies mutual exclusion. A mutex is used to guard shared data such as a linked-list, 
an array or any primitive type. A mutex allows only a single thread to access a resource or critical section.

Once a thread acquires a mutex, all other threads attempting to acquire the same mutex are blocked until the 
first thread releases the mutex. Once released, most implementations arbitrarily chose one of the waiting 
threads to acquire the mutex and make progress.

Semaphore#
Semaphore, on the other hand, is used for limiting access to a collection of resources. Think of semaphore 
s having a limited number of permits to give out. If a semaphore has given out all the permits it has, then 
any new thread that comes along requesting for a permit will be blocked, till an earlier thread with a permit 
returns it to the semaphore. A typical example would be a pool of database connections that can be handed out 
to requesting threads. Say there are ten available connections but 50 requesting threads. In such a scenario, 
a semaphore can only give out ten permits or connections at any given point in time.

A semaphore with a single permit is called a binary semaphore and is often thought of as an equivalent of a 
mutex, which isn't completely correct as we'll shortly explain. Semaphores can also be used for signaling 
among threads. This is an important distinction as it allows threads to cooperatively work towards completing 
a task. A mutex, on the other hand, is strictly limited to serializing access to shared state among competing 
threads.

-------------------------------------------------------------------------------------------------------------------

Difference between Mutex and Semaphore#

Mutex implies mutual exclusion and is used to serialize access to critical sections whereas semaphore can 
potentially be used as a mutex but it can also be used for cooperation and signaling amongst threads. 
Semaphore also solves the issue of missed signals.

Mutex is owned by a thread, whereas a semaphore has no concept of ownership.

Mutex if locked, must necessarily be unlocked by the same thread. A semaphore can be acted upon by different 
threads. This is true even if the semaphore has a permit of one

Think of semaphore analogous to a car rental service such as Hertz. Each outlet has a certain number of cars, 
it can rent out to customers. It can rent several cars to several customers at the same time but if all the 
cars are rented out then any new customers need to be put on a waitlist till one of the rented cars is returned. 
In contrast, think of a mutex like a lone runway on a remote airport. Only a single jet can land or take-off 
from the runway at a given point in time. No other jet can use the runway simultaneously with the first aircraft.

-----------------------------------------------------------------------------------------------------------------------

Java’s Monitor#
In Java every object is a condition variable and has an associated lock that is hidden from the developer. 
Each java object exposes wait() and notify() methods.

Before we execute wait() on a java object we need to lock its hidden mutex. That is done implicitly through 
the synchronized keyword. If you attempt to call wait() or notify() outside of a synchronized block, an 
IllegalMonitorStateException would occur. It's Java reminding the developer that the mutex wasn't acquired 
before wait on the condition variable was invoked. wait() and notify() can only be called on an object once 
the calling thread becomes the owner of the monitor. The ownership of the monitor can be achieved in the 
following ways:

1) the method the thread is executing has synchronized in its signature

2) the thread is executing a block that is synchronized on the object on which wait or notify will be called

3) in case of a class, the thread is executing a static method which is synchronized.

----------------------------------------------------------------------------------------------------------------------

Thread Safe#
A class and its public APIs are labelled as thread safe if multiple threads can consume the exposed APIs without 
causing race conditions or state corruption for the class. Note that composition of two or more thread-safe classes 
doesn't guarantee the resulting type to be thread-safe.

Synchronized#
Java’s most fundamental construct for thread synchronization is the synchronized keyword. It can be used to 
restrict access to critical sections one thread at a time.

Each object in Java has an entity associated with it called the "monitor lock" or just monitor. Think of it as an 
exclusive lock. Once a thread gets hold of the monitor of an object, it has exclusive access to all the methods 
marked as synchronized. No other thread will be allowed to invoke a method on the object that is marked as 
synchronized and will block, till the first thread releases the monitor which is equivalent of the first 
thread exiting the synchronized method.

Note carefully:

For static methods, the monitor will be the class object, which is distinct from the monitor of each instance 
of the same class.

If an uncaught exception occurs in a synchronized method, the monitor is still released.

Furthermore, synchronized blocks can be re-entered.

-----------------------------------------------------------------------------------------------------------------------------

Monitor

Usually, the implementation of monitors is faster/light-weight, since it is designed for multi-threaded synchronization 
within the same process. Also, usually, it is provided by a framework/library itself (as opposed to requesting the OS).

Mutex

Usually, mutexes are provided by the OS kernel and libraries/frameworks simply provide an interface to invoke it. This 
makes them heavy-weight/slower, but they work across threads on different processes. OS might also provide features to 
access the mutex by name for easy sharing between instances of separate executables (as opposed to using a handle that 
can be used by fork only).

-----------------------------------------------------------------------------------------------------------------------------

wait()#
The wait method is exposed on each java object. Each Java object can act as a condition variable. When a thread executes 
the wait method, it releases the monitor for the object and is placed in the wait queue. Note that the thread must be 
inside a synchronized block of code that synchronizes on the same object as the one on which wait() is being called, or 
in other words, the thread must hold the monitor of the object on which it'll call wait. If not so, an illegalMonitor 
exception is raised!

notify()#
Like the wait method, notify() can only be called by the thread which owns the monitor for the object on which notify() 
is being called else an illegal monitor exception is thrown. The notify method, will awaken one of the threads in the 
associated wait queue, i.e., waiting on the thread's monitor.

However, this thread will not be scheduled for execution immediately and will compete with other active threads that 
are trying to synchronize on the same object. The thread which executed notify will also need to give up the object's 
monitor, before any one of the competing threads can acquire the monitor and proceed forward.

notifyAll()#
This method is the same as the notify() one except that it wakes up all the threads that are waiting on the object's 
monitor

------------------------------------------------------------------------------------------------------------------------------

Interrupted Exception#
You'll often come across this exception being thrown from functions. When a thread wait()-s or sleep()-s then one way for 
it to give up waiting/sleeping is to be interrupted. If a thread is interrupted while waiting/sleeping, it'll wake up and 
immediately throw Interrupted exception.

The thread class exposes the interrupt() method which can be used to interrupt a thread that is blocked in a sleep() 
or wait() call. Note that invoking the interrupt method only sets a flag that is polled periodically by sleep or wait 
to know the current thread has been interrupted and an interrupted exception should be thrown.

-------------------------------------------------------------------------------------------------------------------------------

Volatile keyword#

The volatile concept is specific to Java. Its easier to understand volatile, if you understand the problem it solves.

If you have a variable say a counter that is being worked on by a thread, it is possible the thread keeps a copy of 
the counter variable in the CPU cache and manipulates it rather than writing to the main memory. The JVM will decide 
when to update the main memory with the value of the counter, even though other threads may read the value of the 
counter from the main memory and may end up reading a stale value.

If a variable is declared volatile then whenever a thread writes or reads to the volatile variable, the read and 
write always happen in the main memory. As a further guarantee, all the variables that are visible to the writing 
thread also get written-out to the main memory alongside the volatile variable. Similarly, all the variables visible 
to the reading thread alongside the volatile variable will have the latest values visible to the reading thread.

------------------------------------------------------------------------------------------------------------------------------

When is volatile thread-safe#
Volatile comes into play because of multiples levels of memory in hardware architecture required for performance 
enhancements. If there's a single thread that writes to the volatile variable and other threads only read the volatile 
variable then just using volatile is enough, however, if there's a possibility of multiple threads writing to the volatile 
variable then "synchronized" would be required to ensure atomic writes to the variable.

-------------------------------------------------------------------------------------------------------------------------------

What is thread and daemon thread?
Image result for what is a daemon thread
Daemon threads are low priority threads which always run in background and user threads are high priority threads 
which always run in foreground. User Thread or Non-Daemon are designed to do specific or complex task where as daemon 
threads are used to perform supporting tasks

Daemon thread in Java is a low-priority thread that runs in the background to perform tasks such as garbage collection. 
Daemon thread in Java is also a service provider thread that provides services to the user thread. Its life depends on 
the mercy of user threads i.e. when all the user threads die, JVM terminates this thread automatically.

In simple words, we can say that it provides services to user threads for background supporting tasks. It has no role in 
life other than to serve user threads.

Example of Daemon Thread in Java: Garbage collection in Java (gc), finalizer, etc.

Properties of Java Daemon Thread

They can not prevent the JVM from exiting when all the user threads finish their execution.
JVM terminates itself when all user threads finish their execution.
If JVM finds a running daemon thread, it terminates the thread and, after that, shutdown it. JVM does not care 
whether the Daemon thread is running or not.
It is an utmost low priority thread.
Default Nature of Daemon Thread
By default, the main thread is always non-daemon but for all the remaining threads, daemon nature will be inherited 
from parent to child. That is, if the parent is Daemon, the child is also a Daemon and if the parent is a non-daemon, 
then the child is also a non-daemon.

---------------------------------------------------------------------------------------------------------------------------------

Task#
A task is a logical unit of work. Usually, a task should be independent of other tasks so that it can be completed by a 
single thread. A task can be represented by an object of a class implementing the Runnable interface. We can consider 
HTTP requests being fielded by a web-server as tasks that need to be processed. A database server handling client queries 
can similarly be thought of as independent tasks.

Executor Framework#
In Java, the primary abstraction for executing logical tasks units is the Executor framework and not the Thread class. 
The classes in the Executor framework separate out:

Task Submission
Task Execution
The framework allows us to specify different policies for task execution. Java offers three interfaces, which classes 
can implement to manage thread lifecycle. These are:

Executor Interface
ExecutorService
ScheduledExecutorService
The Executor interface forms the basis for the asynchronous task execution framework in Java.

You don't need to create your own executor class as Java's java.util.concurrent package offers several types of 
executors that are suitable for different scenarios. However, as an example, we create a dumb executor which implements 
the Executor Interface.

----------------------------------------------------------------------------------------------------------------------------------

